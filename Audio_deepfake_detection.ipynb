{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pip install datasets==2.14.5"
      ],
      "metadata": {
        "id": "uoT5blq5Kj22",
        "outputId": "387d5fc0-4245-4850-a27c-31bfb2f7fe4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets==2.14.5 in /usr/local/lib/python3.11/dist-packages (2.14.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.5) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.5) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.5) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.5) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.5) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.5) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.5) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.5) (0.70.15)\n",
            "Requirement already satisfied: fsspec<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2023.9.0,>=2023.1.0->datasets==2.14.5) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.5) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.5) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.5) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.14.5) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.5) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.5) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.5) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.5) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.5) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.5) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.14.5) (1.18.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.5) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.5) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.14.5) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.14.5) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.14.5) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.14.5) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.14.5) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.14.5) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.14.5) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.14.5) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "51soszYeFSXi"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import gc\n",
        "import ast\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import torchaudio\n",
        "import sys\n",
        "import datasets\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import AutoFeatureExtractor\n",
        "from transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\n"
      ],
      "metadata": {
        "id": "TpHbTasiHX1E"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_chechpoint = \"facebook/wav2vec2-base\"\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "Pz817hqaIWcU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric = load_metric(\"accuracy\")"
      ],
      "metadata": {
        "id": "Np8YAd6gPxLB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fake_path = os.path.join('/content/dataset/fake data')\n",
        "real_path = os.path.join('/content/dataset/real data')"
      ],
      "metadata": {
        "id": "Mxf_d4BlQLOA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise(data, noise_factor=0.005):\n",
        "    noise = np.random.randn(len(data))\n",
        "    augmented_data = data + noise_factor * noise\n",
        "    return augmented_data\n",
        "\n",
        "def time_stretch(data, rate=1.1):\n",
        "    return librosa.effects.time_strtetch(data, rate)\n",
        "\n",
        "def pitch_shift(data, sr, n_steps=2):\n",
        "    return librosa.effects.pitch_shift(data, sr, n_steps)\n",
        "\n",
        "def extract_features(file_path, augment=False):\n",
        "    y, sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "    if augment:\n",
        "        aug_choice = np.random.choice(['none', 'noise','stretch','pitch'])\n",
        "        if aug_choice == 'noise':\n",
        "            y = add_noise(y)\n",
        "        elif aug_choice == 'stretch':\n",
        "            y = time_stretch(y, rate=np.random.uniform(0.8, 1.2))\n",
        "        elif aug_choice == 'pitch':\n",
        "            y = pitch_shift(y, sr, n_steps=np.random.randint(-3, 3))\n",
        "\n",
        "    # Extract 40 MFCC(Mel-frequency cepstral coefficients)\n",
        "    mfccs = lilbrosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "    mfccs_mean = np.mean(mfccs, axis=1)\n",
        "\n",
        "    # Extract additional features\n",
        "    spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
        "    spectral_bandwidth = np.mean(librosa.feature.spectral_bandwidth(y=y, sr=sr))\n",
        "    spectral_contrast = np.mean(librosa.feature.spectral_contrast(y=y, sr=sr))\n",
        "    spectral_rolloff = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
        "    chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr))\n",
        "    tonnetz = np.mean(librosa.feature.tonnetz(y=y, sr=sr))\n",
        "    zero_crossing_rate = np.mean(librosa.feature.zero_crossing_rate(y))\n",
        "\n",
        "    # Combine all features into single vector\n",
        "    features = np.hstack([mfccs_mean, spectral_centroid, spectral_bandwidth, spectral_contrast, spectral_rolloff, chroma, tonnetz, zero_crossing_rate])\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "npVX5986YIOg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(fake_path, real_path):\n",
        "    x = []\n",
        "    y = []\n",
        "\n",
        "    # Get list of files from both fake and real directories\n",
        "    fake_files = [file for file in os.listdir(fake_path) if file.endswith('.wav')]\n",
        "    real_files = [file for file in os.listdir(real_path) if file.endswith('.wav')]\n",
        "\n",
        "    # Find the minimum number of files between the two classes for balancing\n",
        "    min_files = min(len(fake_files), len(real_files))\n",
        "\n",
        "    print(f\"processing {min_files} 'fake' and {min_files} 'real' audio files for balance\")\n",
        "\n",
        "    # Process fake audio files  (only up to min_files to balance)\n",
        "    for file_name in tqdm(fake_files[:min_files], desc=\"Fake files\", unit=\"file\"):\n",
        "        file_path =os.path.join(fake_path, file)\n",
        "        features = extract_features(file_path)\n",
        "        x.append(features)\n",
        "        y.append(1)  # Label 1 for fake\n",
        "\n",
        "    # Process real audio files( only upto min_files to balance)\n",
        "    for file_name in tqdm(real_files[:min_files], desc=\"Real files\", unit=\"file\"):\n",
        "        file_path = os.path.join(real_path, file_name)\n",
        "        features = extract_features(file_path)\n",
        "        x.append(features)\n",
        "        y.append(0) # Label 0 for real\n",
        "\n",
        "    print(f\"Number of 'fake' files processed: {min_files}\")\n",
        "    print(f\"Number of 'real' files processed: {min_files}\")\n",
        "\n",
        "    return np.array(x), np.array(y), min_files, min_files"
      ],
      "metadata": {
        "id": "kIIUBs6tkvX3"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset with progress bars\n",
        "x, y, fake_files_count, real_files_count = load_dataset(fake_path, real_path)"
      ],
      "metadata": {
        "id": "RDcha4aMo-BC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_labels = ['Fake', 'Real']\n",
        "class_counts = [fake_files_count, real_files_count]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=class_labels, y=class_counts, palette='viridis')\n",
        "plt.title('distribution of fake and real audio files')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of files')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jDLzVj1spSIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators= 100, random_state=42)\n",
        "clf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "aEPz5pBzGf5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100: 2f}%\")\n"
      ],
      "metadata": {
        "id": "2XrS7VlKHpE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TXjouAVKcsvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = clf.feature_importances_\n",
        "feature_names = [f'MFCC {}'i+1 for i in range(40)] + [\n",
        "    'Spectral_Centroid',\n",
        "    'Spectral_Bandwidth',\n",
        "    'Spectral_Contrast',\n",
        "    'Spectral_Rolloff',\n",
        "    'Chroma',\n",
        "    'Tonnetz',\n",
        "    'Zero_Crossing_Rate',\n",
        "    'RMSE'\n",
        "]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x=importances, y=feature_names, palette='magma')\n",
        "plt.title('Feature Importance from Random Foresst Classifier')\n",
        "plt.xlabel('Importance')\n",
        "plt.ylabel('Feature')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3vGNSzmgaraK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "model_filename = \"random forest model.pkl\"\n",
        "joblib.dump(clf, model_filename)\n",
        "print(f\"Model saved as {model_filename}\")\n",
        "\n",
        "loaded_model = joblib.load(model_filename)\n",
        "print(\"model loaded successful\")\n",
        "\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "report = classification_report(y_test, y_pred, target_names=[\"Real\", \"Fake\"])\n",
        "print(\"classification report: \\n\", report)\n",
        "\n",
        "def predict_fake_or_real(wav_file, model):\n",
        "    features = extract_features(wav_file)\n",
        "    features = features.reshape(1, -1)\n",
        "    prediction = model.predict(features)\n",
        "    if prediction[0] == 1:\n",
        "        return \"Fake\"\n",
        "    else:\n",
        "        return \"Real\""
      ],
      "metadata": {
        "id": "WWZzrcjlZB8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, Bidirectional, GlobalMaxPooling1D, Input\n",
        "\n",
        "class_labels = ['Fake', 'Real']\n",
        "class_counts = [fake_files_count, real_files_count]\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.barplot(x=class_labels, y=class_counts, palette='viridis')\n",
        "plt.title('Distribution of Fake and Real Audio Files')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Number of Files')\n",
        "plt.show()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = np.expand_dims(X_train, axis=2)\n",
        "X_test = np.expand_dims(X_test, axis=2)\n",
        "\n",
        "def build_model(input_shape):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    x = Conv1D(filters=64, kernel_size=3, padding='same', activation='relu')(inputs)\n",
        "    x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
        "\n",
        "    attention = Attention()([x, x])\n",
        "    attention = GlobalMaxPooling1D()(attention)\n",
        "\n",
        "    x = Dense(128, activation='relu')(attention)\n",
        "    x = Dropout(0.5)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    model = Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=50, batch_size=64, validation_split=0.2,verbose=1)\n",
        "\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(classification_report(y_test, y_pred, target_names=class_labels))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='Train accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='Train loss')\n",
        "plt.plot(history.history['val_loss'], label='val loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CWh5dvQzK0wA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}